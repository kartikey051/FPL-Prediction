{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (4.39.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (4.14.3)\n",
      "Requirement already satisfied: pandas in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (2.3.3)\n",
      "Requirement already satisfied: webdriver-manager in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3.0,>=2.5.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (2.6.0)\n",
      "Requirement already satisfied: trio<1.0,>=0.31.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from selenium) (0.32.0)\n",
      "Requirement already satisfied: trio-websocket<1.0,>=0.12.2 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2025.10.5 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from selenium) (2025.11.12)\n",
      "Requirement already satisfied: typing_extensions<5.0,>=4.15.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from selenium) (4.15.0)\n",
      "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from selenium) (1.9.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
      "Requirement already satisfied: sortedcontainers in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from trio<1.0,>=0.31.0->selenium) (3.11)\n",
      "Requirement already satisfied: outcome in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from trio-websocket<1.0,>=0.12.2->selenium) (1.3.2)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: requests in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from webdriver-manager) (2.32.5)\n",
      "Requirement already satisfied: python-dotenv in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from webdriver-manager) (1.2.1)\n",
      "Requirement already satisfied: packaging in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from webdriver-manager) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h11<1,>=0.16.0 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/tarun/Documents/Python_Practice/venv/lib/python3.14/site-packages (from requests->webdriver-manager) (3.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium beautifulsoup4 pandas webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting automatic WebDriver setup...\n",
      "‚úÖ Automatic WebDriver setup successful.\n",
      "\n",
      "============================================================\n",
      "STEP 1: Extracting Player Links\n",
      "============================================================\n",
      "Fetching player list from wages page (Attempt 1)...\n",
      "  Page load timeout, but attempting to parse anyway...\n",
      "  üìù Saved page HTML to debug_wages_page.html for inspection\n",
      "  Found 2709 total links on page\n",
      "    Found: Cristiano Ronaldo\n",
      "    Found: Lionel Messi\n",
      "    Found: Rayan Cherki\n",
      "    Found: Manu Kon√©\n",
      "    Found: Erling Haaland\n",
      "  Total unique players found: 633\n",
      "‚úÖ Successfully extracted 633 players\n",
      "\n",
      "Total players to scrape: 633\n",
      "\n",
      "============================================================\n",
      "STEP 2: Scraping Player Match Logs\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/633] Processing: Cristiano Ronaldo\n",
      "--------------------------------------------------\n",
      "  Match log URL: https://fbref.com/en/players/dea698d9/matchlogs/all_comps/\n",
      "  Scraping match logs (Attempt 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q8/rfr14j2j62n1zftp1z06v_bm0000gn/T/ipykernel_4269/3753183786.py:277: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  dfs = pd.read_html(str(table))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Scraped 5 match logs for Cristiano Ronaldo\n",
      "  üíæ Saved individual file: Cristiano_Ronaldo_match_logs.csv\n",
      "  ‚è±Ô∏è  Sleeping for 0.95 seconds...\n",
      "\n",
      "[2/633] Processing: Lionel Messi\n",
      "--------------------------------------------------\n",
      "  Match log URL: https://fbref.com/en/players/d70ce98e/matchlogs/all_comps/\n",
      "  Scraping match logs (Attempt 1)...\n",
      "  ‚ö†Ô∏è  No data extracted for Lionel Messi\n",
      "  ‚è±Ô∏è  Sleeping for 1.41 seconds...\n",
      "\n",
      "[3/633] Processing: Rayan Cherki\n",
      "--------------------------------------------------\n",
      "  Match log URL: https://fbref.com/en/players/b34c63a5/matchlogs/all_comps/\n",
      "  Scraping match logs (Attempt 1)...\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, SessionNotCreatedException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Custom exception for interrupt handling\n",
    "class SigTermException(Exception):\n",
    "    \"\"\"Custom exception to signal the script was interrupted.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Attempt to import webdriver-manager for automatic driver handling\n",
    "try:\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    AUTOMATIC_DRIVER = True\n",
    "except ImportError:\n",
    "    AUTOMATIC_DRIVER = False\n",
    "    print(\"Warning: 'webdriver-manager' not installed. Using manual path only.\")\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='player_scraper_errors.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define script parameters\n",
    "WAGES_URL = \"https://fbref.com/en/comps/9/wages/Premier-League-Wages\"\n",
    "OUTPUT_DIR = \"fbref_player_match_logs\"\n",
    "MANUAL_DRIVER_PATH = r\"C:\\Users\\vanim\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "START_DATE = datetime(2016, 1, 1)  # January 2016\n",
    "\n",
    "# Create base output directory\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# --- WebDriver Initialization ---\n",
    "\n",
    "def initialize_driver():\n",
    "    \"\"\"Initialize Chrome WebDriver with optimal settings.\"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "    \n",
    "    driver = None\n",
    "    try:\n",
    "        if AUTOMATIC_DRIVER:\n",
    "            print(\"Attempting automatic WebDriver setup...\")\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            driver = webdriver.Chrome(service=service, options=options)\n",
    "            print(\"‚úÖ Automatic WebDriver setup successful.\")\n",
    "        else:\n",
    "            raise ImportError(\"webdriver-manager not available.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Automatic setup failed or skipped. Trying manual setup as failsafe.\")\n",
    "        logging.warning(f\"Automatic WebDriver setup failed/skipped: {e}\")\n",
    "\n",
    "        try:\n",
    "            ser = Service(MANUAL_DRIVER_PATH)\n",
    "            driver = webdriver.Chrome(service=ser, options=options)\n",
    "            print(\"‚úÖ Manual WebDriver setup successful.\")\n",
    "\n",
    "        except SessionNotCreatedException as manual_e:\n",
    "            print(f\"‚ùå FATAL ERROR: Manual setup failed. Check if ChromeDriver version matches Chrome browser.\")\n",
    "            logging.critical(f\"FATAL WebDriver Error (Manual): {manual_e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        except WebDriverException as manual_e:\n",
    "            print(f\"‚ùå FATAL ERROR: Manual setup failed. Check the file path: {MANUAL_DRIVER_PATH}\")\n",
    "            logging.critical(f\"FATAL WebDriver Error (Manual): {manual_e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    if driver is None:\n",
    "        print(\"‚ùå FATAL ERROR: Driver could not be initialized. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Set timeouts\n",
    "    driver.set_page_load_timeout(60)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    return driver\n",
    "\n",
    "driver = initialize_driver()\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def extract_player_links(driver, url, max_retries=3):\n",
    "    \"\"\"\n",
    "    Extracts all player profile links from the Premier League wages page.\n",
    "    Returns a list of dictionaries with player name and URL.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Fetching player list from wages page (Attempt {attempt + 1})...\")\n",
    "            \n",
    "            # Try to refresh the driver connection if having issues\n",
    "            if attempt > 0:\n",
    "                print(\"  Refreshing driver session...\")\n",
    "                try:\n",
    "                    driver.refresh()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            driver.get(url)\n",
    "            \n",
    "            # Wait for the wages table to load with a more specific condition\n",
    "            try:\n",
    "                WebDriverWait(driver, 30).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"  Page load timeout, but attempting to parse anyway...\")\n",
    "            \n",
    "            # Give the page a moment to fully render\n",
    "            time.sleep(3)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # DEBUG: Save HTML to file for inspection\n",
    "            if attempt == 0:\n",
    "                with open(\"debug_wages_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html)\n",
    "                print(\"  üìù Saved page HTML to debug_wages_page.html for inspection\")\n",
    "            \n",
    "            players = []\n",
    "            \n",
    "            # Strategy 1: Find ALL links that point to player profiles\n",
    "            all_links = soup.find_all(\"a\", href=True)\n",
    "            print(f\"  Found {len(all_links)} total links on page\")\n",
    "            \n",
    "            player_links_found = 0\n",
    "            for link in all_links:\n",
    "                href = link.get(\"href\", \"\")\n",
    "                # Player URLs follow pattern: /en/players/PLAYER_ID/PLAYER_NAME\n",
    "                if \"/players/\" in href and href.count(\"/\") >= 4:\n",
    "                    player_name = link.text.strip()\n",
    "                    \n",
    "                    # Skip if no name\n",
    "                    if not player_name or len(player_name) < 2:\n",
    "                        continue\n",
    "                    \n",
    "                    # Build full URL\n",
    "                    if not href.startswith(\"http\"):\n",
    "                        player_url = \"https://fbref.com\" + href\n",
    "                    else:\n",
    "                        player_url = href\n",
    "                    \n",
    "                    # Avoid duplicates\n",
    "                    if not any(p[\"name\"] == player_name for p in players):\n",
    "                        players.append({\n",
    "                            \"name\": player_name,\n",
    "                            \"url\": player_url\n",
    "                        })\n",
    "                        player_links_found += 1\n",
    "                        \n",
    "                        # Debug: Print first few players found\n",
    "                        if player_links_found <= 5:\n",
    "                            print(f\"    Found: {player_name}\")\n",
    "            \n",
    "            print(f\"  Total unique players found: {len(players)}\")\n",
    "            \n",
    "            if players:\n",
    "                print(f\"‚úÖ Successfully extracted {len(players)} players\")\n",
    "                return players\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  No players extracted. Retrying...\")\n",
    "                \n",
    "                # DEBUG: Print sample of what we found\n",
    "                print(\"  Debug - Sample links found:\")\n",
    "                sample_links = [l.get(\"href\") for l in all_links[:10] if l.get(\"href\")]\n",
    "                for sl in sample_links:\n",
    "                    print(f\"    {sl}\")\n",
    "                \n",
    "                time.sleep(3)\n",
    "                continue\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logging.error(f\"Timeout fetching player list (Attempt {attempt + 1})\")\n",
    "            print(f\"  Timeout (Attempt {attempt + 1}). Retrying...\")\n",
    "            time.sleep(5)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error extracting player links (Attempt {attempt + 1}): {str(e)}\")\n",
    "            print(f\"  Error (Attempt {attempt + 1}): {str(e)}\")\n",
    "            import traceback\n",
    "            print(f\"  Traceback: {traceback.format_exc()}\")\n",
    "            time.sleep(3)\n",
    "    \n",
    "    print(f\"‚ùå Failed to extract player links after {max_retries} attempts.\")\n",
    "    print(f\"   Please check debug_wages_page.html to see the actual page content.\")\n",
    "    return []\n",
    "\n",
    "def get_match_log_url(driver, player_url, max_retries=3):\n",
    "    \"\"\"\n",
    "    Navigates to a player's profile and extracts the full match log URL.\n",
    "    Returns the match log URL or None if not found.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            driver.get(player_url)\n",
    "            \n",
    "            # Wait for page to load\n",
    "            time.sleep(2)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Look for \"Match Logs\" link\n",
    "            match_log_links = soup.find_all(\"a\", href=True)\n",
    "            for link in match_log_links:\n",
    "                if \"matchlogs\" in link[\"href\"] and \"all_comps\" in link[\"href\"]:\n",
    "                    full_url = \"https://fbref.com\" + link[\"href\"] if link[\"href\"].startswith(\"/\") else link[\"href\"]\n",
    "                    return full_url\n",
    "            \n",
    "            # Strategy 2: Construct URL from player ID\n",
    "            player_id = player_url.split(\"/players/\")[1].split(\"/\")[0] if \"/players/\" in player_url else None\n",
    "            \n",
    "            if player_id:\n",
    "                constructed_url = f\"https://fbref.com/en/players/{player_id}/matchlogs/all_comps/\"\n",
    "                return constructed_url\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except TimeoutException:\n",
    "            logging.error(f\"Timeout getting match log URL (Attempt {attempt + 1})\")\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error getting match log URL (Attempt {attempt + 1}): {str(e)}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return None\n",
    "\n",
    "def scrape_match_logs(driver, match_log_url, player_name, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrapes all match logs for a player from their match log page.\n",
    "    Filters data from January 2016 onwards.\n",
    "    Returns a DataFrame with all match log data.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"  Scraping match logs (Attempt {attempt + 1})...\")\n",
    "            driver.get(match_log_url)\n",
    "            \n",
    "            # Wait and give page time to load\n",
    "            time.sleep(2)\n",
    "            \n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # Find the match logs table - try multiple possible IDs\n",
    "            table = None\n",
    "            for table_id_pattern in [\"matchlogs_all\", \"matchlogs\", \"matchlogs_for\"]:\n",
    "                tables = soup.find_all(\"table\", {\"id\": lambda x: x and table_id_pattern in x})\n",
    "                if tables:\n",
    "                    table = tables[0]\n",
    "                    break\n",
    "            \n",
    "            if table is None:\n",
    "                logging.warning(f\"Match logs table not found for {player_name}\")\n",
    "                return None\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            try:\n",
    "                dfs = pd.read_html(str(table))\n",
    "                df = dfs[0] if dfs else None\n",
    "                \n",
    "                if df is None or df.empty:\n",
    "                    logging.warning(f\"Empty DataFrame for {player_name}\")\n",
    "                    return None\n",
    "                \n",
    "                # Handle multi-level column headers if present\n",
    "                if isinstance(df.columns, pd.MultiIndex):\n",
    "                    df.columns = ['_'.join(col).strip() for col in df.columns.values]\n",
    "                \n",
    "                # Add player identification\n",
    "                df['Player_Name'] = player_name\n",
    "                df['Player_URL'] = match_log_url\n",
    "                \n",
    "                # Filter by date if Date column exists\n",
    "                date_columns = [col for col in df.columns if 'date' in col.lower()]\n",
    "                if date_columns:\n",
    "                    date_col = date_columns[0]\n",
    "                    try:\n",
    "                        df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "                        df = df[df[date_col] >= START_DATE]\n",
    "                        df = df.dropna(subset=[date_col])\n",
    "                    except Exception as e:\n",
    "                        logging.warning(f\"Could not filter by date for {player_name}: {str(e)}\")\n",
    "                \n",
    "                print(f\"  ‚úÖ Scraped {len(df)} match logs for {player_name}\")\n",
    "                return df\n",
    "                \n",
    "            except ValueError as e:\n",
    "                logging.error(f\"Error reading HTML table for {player_name}: {str(e)}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "                \n",
    "        except TimeoutException:\n",
    "            logging.error(f\"Timeout scraping match logs for {player_name} (Attempt {attempt + 1})\")\n",
    "            print(f\"  Timeout (Attempt {attempt + 1}). Retrying...\")\n",
    "            time.sleep(3)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error scraping match logs for {player_name} (Attempt {attempt + 1}): {str(e)}\")\n",
    "            print(f\"  Error (Attempt {attempt + 1}): {str(e)}. Retrying...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"  ‚ùå Failed to scrape match logs for {player_name} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "def consolidate_data(all_data_frames, output_dir):\n",
    "    \"\"\"Consolidates all collected DataFrames and saves them to a single CSV.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Starting data consolidation...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if all_data_frames:\n",
    "        consolidated_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        consolidated_filename = os.path.join(output_dir, \"All_Player_Match_Logs_Consolidated.csv\")\n",
    "        consolidated_df.to_csv(consolidated_filename, index=False)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Consolidation successful!\")\n",
    "        print(f\"   Total match logs: {len(consolidated_df)}\")\n",
    "        print(f\"   Unique players: {consolidated_df['Player_Name'].nunique()}\")\n",
    "        print(f\"   Output file: {consolidated_filename}\")\n",
    "        print(\"\\nSample of consolidated data:\")\n",
    "        print(consolidated_df.head(10).to_string())\n",
    "        \n",
    "        return consolidated_df\n",
    "    else:\n",
    "        print(\"‚ùå No data was successfully scraped to consolidate.\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Loop ---\n",
    "\n",
    "all_data_frames = []\n",
    "successful_players = 0\n",
    "failed_players = 0\n",
    "\n",
    "try:\n",
    "    # Step 1: Extract player links from wages page\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 1: Extracting Player Links\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    players = extract_player_links(driver, WAGES_URL)\n",
    "    \n",
    "    if not players:\n",
    "        print(\"‚ùå No players found. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(f\"\\nTotal players to scrape: {len(players)}\")\n",
    "    \n",
    "    # Step 2: Scrape match logs for each player\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STEP 2: Scraping Player Match Logs\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for i, player in enumerate(players, 1):\n",
    "        print(f\"\\n[{i}/{len(players)}] Processing: {player['name']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            # Get match log URL\n",
    "            match_log_url = get_match_log_url(driver, player['url'])\n",
    "            \n",
    "            if not match_log_url:\n",
    "                print(f\"  ‚ö†Ô∏è  Could not find match log URL for {player['name']}\")\n",
    "                failed_players += 1\n",
    "                continue\n",
    "            \n",
    "            print(f\"  Match log URL: {match_log_url}\")\n",
    "            \n",
    "            # Scrape match logs\n",
    "            player_df = scrape_match_logs(driver, match_log_url, player['name'])\n",
    "            \n",
    "            if player_df is not None and not player_df.empty:\n",
    "                all_data_frames.append(player_df)\n",
    "                successful_players += 1\n",
    "                \n",
    "                # Save individual player file\n",
    "                safe_name = player['name'].replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "                individual_file = os.path.join(OUTPUT_DIR, f\"{safe_name}_match_logs.csv\")\n",
    "                player_df.to_csv(individual_file, index=False)\n",
    "                print(f\"  üíæ Saved individual file: {safe_name}_match_logs.csv\")\n",
    "            else:\n",
    "                print(f\"  ‚ö†Ô∏è  No data extracted for {player['name']}\")\n",
    "                failed_players += 1\n",
    "            \n",
    "            # Random delay between requests (0.8 to 2.0 seconds)\n",
    "            sleep_time = random.uniform(0.8, 2.0)\n",
    "            print(f\"  ‚è±Ô∏è  Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error processing {player['name']}: {str(e)}\")\n",
    "            print(f\"  ‚ùå Error processing {player['name']}: {str(e)}\")\n",
    "            failed_players += 1\n",
    "            continue\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n*** SCRIPT INTERRUPTED BY USER (Ctrl+C)! ***\")\n",
    "    logging.info(\"Script interrupted by user.\")\n",
    "    raise SigTermException(\"User interrupted scraping.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Critical error in main loop: {str(e)}\")\n",
    "    print(f\"\\n‚ùå CRITICAL SCRIPT ERROR: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Consolidation and cleanup\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL CONSOLIDATION & CLEANUP\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nüìä Scraping Summary:\")\n",
    "        print(f\"   Successful: {successful_players}\")\n",
    "        print(f\"   Failed: {failed_players}\")\n",
    "        print(f\"   Total attempted: {successful_players + failed_players}\")\n",
    "        \n",
    "        consolidate_data(all_data_frames, OUTPUT_DIR)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during final consolidation: {str(e)}\")\n",
    "        logging.error(f\"Error during final consolidation: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nüîí Closing browser...\")\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"\\n‚úÖ Script execution completed!\")\n",
    "    print(f\"üìÅ All output saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.14 (Practice)",
   "language": "python",
   "name": "python_practice"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
