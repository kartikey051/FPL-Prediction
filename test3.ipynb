{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336d0e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting automatic WebDriver setup...\n",
      "✅ Automatic WebDriver setup successful.\n",
      "Created directory: fbref_premier_league_data_insider01\\2015-2016\n",
      "Scraping (Attempt 1): Liverpool for 2015-2016\n",
      "Timeout for Liverpool in 2015-2016 (Attempt 1). Retrying.\n",
      "Scraping (Attempt 2): Liverpool for 2015-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanim\\AppData\\Local\\Temp\\ipykernel_30480\\3944627956.py:145: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved individual file: fbref_premier_league_data_insider01\\2015-2016\\Liverpool_PremierLeague.csv\n",
      "Sleeping for 1.23 seconds...\n",
      "Scraping (Attempt 1): Arsenal for 2015-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanim\\AppData\\Local\\Temp\\ipykernel_30480\\3944627956.py:145: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved individual file: fbref_premier_league_data_insider01\\2015-2016\\Arsenal_PremierLeague.csv\n",
      "Sleeping for 0.76 seconds...\n",
      "Scraping (Attempt 1): Manchester-City for 2015-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanim\\AppData\\Local\\Temp\\ipykernel_30480\\3944627956.py:145: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved individual file: fbref_premier_league_data_insider01\\2015-2016\\Manchester-City_PremierLeague.csv\n",
      "Sleeping for 1.16 seconds...\n",
      "Scraping (Attempt 1): Chelsea for 2015-2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vanim\\AppData\\Local\\Temp\\ipykernel_30480\\3944627956.py:145: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  df = pd.read_html(str(table))[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved individual file: fbref_premier_league_data_insider01\\2015-2016\\Chelsea_PremierLeague.csv\n",
      "Sleeping for 0.99 seconds...\n",
      "Scraping (Attempt 1): Newcastle-Utd for 2015-2016\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException, SessionNotCreatedException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import random # Import random for variable sleep times\n",
    "\n",
    "# Custom exception for interrupt handling\n",
    "class SigTermException(Exception):\n",
    "    \"\"\"Custom exception to signal the script was interrupted.\"\"\"\n",
    "    pass\n",
    "\n",
    "# Attempt to import webdriver-manager for automatic driver handling\n",
    "try:\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "    AUTOMATIC_DRIVER = True\n",
    "except ImportError:\n",
    "    AUTOMATIC_DRIVER = False\n",
    "    print(\"Warning: 'webdriver-manager' not installed. Using manual path only.\")\n",
    "\n",
    "\n",
    "# --- Configuration and Setup ---\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename='scraper_errors.log', level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define script parameters\n",
    "START_YEAR = 2015\n",
    "END_YEAR = 2024\n",
    "OUTPUT_DIR = \"fbref_premier_league_data_insider01\"\n",
    "# Manual driver path (Failsafe)\n",
    "MANUAL_DRIVER_PATH = r\"C:\\Users\\vanim\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\"\n",
    "\n",
    "# Create base output directory\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "# Full list of relevant Premier League teams/squads\n",
    "TEAMS_FULL = [\n",
    "    \"Liverpool\", \"Arsenal\", \"Manchester-City\", \"Chelsea\", \"Newcastle-Utd\",\n",
    "    \"Aston-Villa\", \"Nott'ham-Forest\", \"Brighton\", \"Bournemouth\", \"Brentford\",\n",
    "    \"Fulham\", \"Crystal-Palace\", \"Everton\", \"West-Ham\", \"Manchester-Utd\",\n",
    "    \"Wolves\", \"Tottenham\", \"Leicester-City\", \"Ipswich-Town\", \"Southampton\",\"Middlesbrough\",\"Hull-City\",\"Burnley\",\"Swansea-City\",\"Stoke-City\",\"West-Bromwich-Albion\",\n",
    "    \"Huddersfield-Town\",\"Norwich-City\",\"Sheffield-United\",\"Cardiff-City\",\"Leeds-United\",\"Luton-Town\",\"Watford\",\"Sunderland\",\"Leicester\"]\n",
    "\n",
    "# Squad IDs map\n",
    "SQUAD_IDS = {\n",
    "    \"Liverpool\": \"822bd0ba\", \"Arsenal\": \"18bb7c10\", \"Manchester-City\": \"b8fd03ef\", \"Chelsea\": \"cff3d9bb\",\n",
    "    \"Newcastle-Utd\": \"b2b47a98\", \"Aston-Villa\": \"8602292d\", \"Nott'ham-Forest\": \"e4a775cb\", \"Brighton\": \"d07537b9\",\n",
    "    \"Bournemouth\": \"4ba7cbea\", \"Brentford\": \"cd051869\", \"Fulham\": \"fd962109\", \"Crystal-Palace\": \"47c64c55\",\n",
    "    \"Everton\": \"d3fd31cc\", \"West-Ham\": \"7c21e445\", \"Manchester-Utd\": \"19538871\", \"Wolves\": \"8cec06e1\",\n",
    "    \"Tottenham\": \"361ca564\", \"Leicester-City\": \"a2fb4471\", \"Ipswich-Town\": \"b74092de\", \"Southampton\": \"33c895d4\",\n",
    "    \"Norwich-City\": \"1c781004\", \"Stoke-City\": \"17892952\", \"Swansea-City\": \"fb10988f\", \"Watford\": \"2abfe087\",\n",
    "    \"Sunderland\": \"8ef52968\", \"Leicester\": \"a2fb4471\", \"Middlesbrough\": \"7f59c601\", \"Hull-City\": \"bd8769d1\",\n",
    "    \"Burnley\": \"943e8050\", \"West-Bromwich-Albion\": \"60c6b05f\", \"Huddersfield-Town\": \"f5922ca5\",\n",
    "    \"Sheffield-United\": \"1df6b87e\", \"Cardiff-City\": \"75fae011\", \"Leeds-United\": \"5bfb9659\", \"Luton-Town\": \"e297cd13\",\n",
    "}\n",
    "\n",
    "# --- WebDriver Initialization (Automatic -> Manual Failsafe) ---\n",
    "\n",
    "driver = None\n",
    "try:\n",
    "    if AUTOMATIC_DRIVER:\n",
    "        print(\"Attempting automatic WebDriver setup...\")\n",
    "        # 1. Automatic attempt using webdriver-manager\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        print(\"✅ Automatic WebDriver setup successful.\")\n",
    "    else:\n",
    "        # Skip straight to manual if package is missing\n",
    "        raise ImportError(\"webdriver-manager not available.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Automatic setup failed or skipped. Trying manual setup as failsafe.\")\n",
    "    logging.warning(f\"Automatic WebDriver setup failed/skipped: {e}\")\n",
    "\n",
    "    try:\n",
    "        # 2. Manual Failsafe attempt\n",
    "        ser = Service(MANUAL_DRIVER_PATH)\n",
    "        driver = webdriver.Chrome(service=ser)\n",
    "        print(\"✅ Manual WebDriver setup successful.\")\n",
    "\n",
    "    except SessionNotCreatedException as manual_e:\n",
    "        print(f\"❌ FATAL ERROR: Manual setup failed. Check if ChromeDriver version matches Chrome browser.\")\n",
    "        logging.critical(f\"FATAL WebDriver Error (Manual): {manual_e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    except WebDriverException as manual_e:\n",
    "        print(f\"❌ FATAL ERROR: Manual setup failed. Check the file path: {MANUAL_DRIVER_PATH}\")\n",
    "        logging.critical(f\"FATAL WebDriver Error (Manual): {manual_e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# Final check to ensure driver is ready\n",
    "if driver is None:\n",
    "    print(\"❌ FATAL ERROR: Driver could not be initialized by either method. Exiting.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# --- Scraping Function ---\n",
    "\n",
    "def scrape_team(driver, url, team, season, season_dir, max_retries=3):\n",
    "    \"\"\"\n",
    "    Scrapes a team's stats, saves it to a CSV, and returns the DataFrame.\n",
    "    Includes robust error handling for network/site issues.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(f\"Scraping (Attempt {attempt + 1}): {team} for {season}\")\n",
    "\n",
    "            # Set a page load timeout to catch slow/dead websites\n",
    "            driver.set_page_load_timeout(45)\n",
    "            driver.get(url)\n",
    "\n",
    "            # Wait for the main table to load\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//table[contains(@id, 'stats_standard')]\"))\n",
    "            )\n",
    "\n",
    "            # Parse page source\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Find the table (try multiple possible IDs)\n",
    "            table = None\n",
    "            for table_id in [\"stats_standard_9\", \"stats_standard\", \"stats_standard_12\"]:\n",
    "                table = soup.find(\"table\", {\"id\": table_id})\n",
    "                if table:\n",
    "                    break\n",
    "\n",
    "            if table is None:\n",
    "                logging.warning(f\"Stats table not found for {team} in {season} at {url}. Skipping.\")\n",
    "                return None\n",
    "\n",
    "            # Convert to DataFrame\n",
    "            try:\n",
    "                # pandas read_html is excellent for this task\n",
    "                df = pd.read_html(str(table))[0]\n",
    "            except ValueError:\n",
    "                logging.error(f\"Error reading HTML table into DataFrame for {team} in {season}. Retrying.\")\n",
    "                time.sleep(2) # Keep this sleep for error retries\n",
    "                continue\n",
    "\n",
    "            # Add 'Team' and 'Season' columns for consolidation\n",
    "            df['Team'] = team\n",
    "            df['Season'] = season\n",
    "\n",
    "            # Save the individual file\n",
    "            filename = os.path.join(season_dir, f\"{team}_PremierLeague.csv\")\n",
    "            df.to_csv(filename, index=False)\n",
    "            print(f\"Saved individual file: {filename}\")\n",
    "\n",
    "            return df # Return the DataFrame for consolidation\n",
    "\n",
    "        except TimeoutException:\n",
    "            logging.error(f\"Timeout (45s page load) for {team} in {season} (Attempt {attempt + 1}). Website might be slow or down.\")\n",
    "            print(f\"Timeout for {team} in {season} (Attempt {attempt + 1}). Retrying.\")\n",
    "            time.sleep(5) # Keep this sleep for network/site issues\n",
    "        except WebDriverException as e:\n",
    "            # Catches network errors (site down, connection lost, etc.)\n",
    "            logging.error(f\"WebDriver/Network error for {team} in {season} (Attempt {attempt + 1}): {str(e)}\")\n",
    "            print(f\"WebDriver/Network error for {team} in {season} (Attempt {attempt + 1}). Retrying.\")\n",
    "            time.sleep(5) # Keep this sleep for network/site issues\n",
    "        except Exception as e:\n",
    "            # Catch all other unexpected errors\n",
    "            logging.error(f\"Unexpected error scraping {team} for {season} (Attempt {attempt + 1}): {str(e)}\")\n",
    "            print(f\"Unexpected error scraping {team} for {season} (Attempt {attempt + 1}). Retrying.\")\n",
    "            time.sleep(2) # Keep this sleep for general errors\n",
    "\n",
    "    print(f\"Failed to scrape {team} for {season} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# --- Consolidation Function (Pulled out for reuse on interrupt) ---\n",
    "\n",
    "def consolidate_data(all_data_frames, output_dir):\n",
    "    \"\"\"Consolidates all collected DataFrames and saves them to a single CSV.\"\"\"\n",
    "    print(\"\\nStarting data consolidation...\")\n",
    "    if all_data_frames:\n",
    "        consolidated_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "        consolidated_filename = os.path.join(output_dir, \"All_Premier_League_Stats_Consolidated.csv\")\n",
    "        consolidated_df.to_csv(consolidated_filename, index=False)\n",
    "        print(f\"Consolidation successful! Saved all data to: {consolidated_filename}\")\n",
    "        print(f\"\\n✅ Data saved in {len(consolidated_df)} rows.\")\n",
    "        print(\"\\nHead of Consolidated Data (Partial/Full Output):\")\n",
    "        print(consolidated_df.head())\n",
    "        # Return the DataFrame for immediate inspection in the testing phase\n",
    "        return consolidated_df\n",
    "    else:\n",
    "        print(\"No data was successfully scraped to consolidate.\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Loop ---\n",
    "\n",
    "all_data_frames = []\n",
    "\n",
    "try:\n",
    "    for year in range(START_YEAR, END_YEAR + 1):\n",
    "        next_year = year + 1\n",
    "        season = f\"{year}-{next_year}\"\n",
    "\n",
    "        # Create season subdirectory\n",
    "        season_dir = os.path.join(OUTPUT_DIR, season)\n",
    "        if not os.path.exists(season_dir):\n",
    "            os.makedirs(season_dir)\n",
    "            print(f\"Created directory: {season_dir}\")\n",
    "\n",
    "        for team in TEAMS_FULL:\n",
    "            squad_id = SQUAD_IDS.get(team)\n",
    "            if not squad_id:\n",
    "                print(f\"No squad ID for {team} in {season}. Skipping...\")\n",
    "                logging.error(f\"No squad ID for {team} in {season}\")\n",
    "                continue\n",
    "\n",
    "            url = f\"https://fbref.com/en/squads/{squad_id}/{season}/{team}-Stats\"\n",
    "\n",
    "            team_df = scrape_team(driver, url, team, season, season_dir)\n",
    "\n",
    "            if team_df is not None:\n",
    "                all_data_frames.append(team_df)\n",
    "\n",
    "            # AGGRESSIVELY REDUCED AND RANDOMIZED SLEEP TIME\n",
    "            # Use random float sleep for aggressive, but variable, rate\n",
    "            sleep_time = random.uniform(0.5, 1.5)\n",
    "            print(f\"Sleeping for {sleep_time:.2f} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    # Handle user-initiated interrupt (Ctrl+C)\n",
    "    print(\"\\n\\n*** SCRIPT INTERRUPTED BY USER (Ctrl+C)! ***\")\n",
    "    logging.info(\"Script interrupted by user.\")\n",
    "    # Raise custom exception to bypass the rest of the standard 'try' block\n",
    "    raise SigTermException(\"User interrupted scraping.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.critical(f\"A critical error occurred in the main scraping loop: {str(e)}\")\n",
    "    print(f\"\\nCRITICAL SCRIPT ERROR: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # This block executes even after a normal run, an exception, or a KeyboardInterrupt\n",
    "    try:\n",
    "        # Check if the interruption was manual before consolidation\n",
    "        if isinstance(sys.exc_info()[1], SigTermException) or not all_data_frames:\n",
    "            # If interrupted, consolidate what we have now\n",
    "            print(\"Processing partially scraped data before exiting...\")\n",
    "        \n",
    "        # Consolidation and saving is done here, making it interrupt-proof\n",
    "        consolidate_data(all_data_frames, OUTPUT_DIR)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during final consolidation/cleanup: {str(e)}\")\n",
    "        logging.error(f\"Error during final consolidation/cleanup: {str(e)}\")\n",
    "\n",
    "    print(\"Closing browser...\")\n",
    "    # Ensures the driver is closed cleanly\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032bfca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
